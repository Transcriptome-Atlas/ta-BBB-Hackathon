{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query vLLM\n",
    "\n",
    "This notebook shows how to use llama_index to query vLLM.\n",
    "\n",
    "We will create a simple vector store from a document and retrieve the relevant information. Then we will query vLLM using a pydantic schema (`Schema.Article`), to get structured json output.\n",
    "\n",
    "Let's start with the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from rich.pretty import pprint\n",
    "from Schema import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up some parameters, that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "model_ds = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_ds_nm = \"neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8\"\n",
    "model_qw = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model_used = model_ds_nm\n",
    "\n",
    "embed_sm = \"BAAI/bge-small-en-v1.5\"\n",
    "embed_used = embed_sm\n",
    "\n",
    "article_path = \"./input/ifnar2.pdf\"\n",
    "\n",
    "api_base_vllm = \"http://localhost:8000/v1\"\n",
    "api_key = \"fake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will set up the vLLM client. We will also set up the embedding model, which is used to store and retrieve text in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vLLM and embedding setup\n",
    "\n",
    "llm=OpenAILike(model=model_used, api_base=api_base_vllm, api_key=api_key)\n",
    "client = llm._get_client()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embed_used\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a vector store from a given document. From the vector store, we will retrieve all data relevant for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "\n",
    "pdf_reader = PDFReader()\n",
    "documents = pdf_reader.load_data(file=Path(article_path))\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "query_text = \"IFNAR2 gene transcript isoforms\"\n",
    "retriever = index.as_retriever()\n",
    "nodes = retriever.retrieve(query_text)\n",
    "retrieved_text = \"\\n\".join([str(node.text) for node in nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will query vLLM with a system promt and user prompt including the text retrieved from the document.\n",
    "\n",
    "Note, that we enforce a structured response according to the pydantic schema (defined in `Schema.Article`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query vLLM\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=model_used,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Fill in the IFNAR2 gene transcript isoforms using the following text: {retrieved_text}\"},\n",
    "    ],\n",
    "    response_format=Article,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the response in a nice, human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output response\n",
    "\n",
    "message = completion.choices[0].message\n",
    "if message.parsed:\n",
    "    pprint(message.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
